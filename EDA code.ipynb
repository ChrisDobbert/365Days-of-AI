{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "* [Classifying Iris Species](#iris)\n",
    " * [Meet the data (EDA)](#iris_data)\n",
    " * [Scatter Matrix](#Scatter_Matrix)\n",
    " * [Distributions](#Distributions)\n",
    " * [Boxplots](#Boxplots)\n",
    " * [Radviz](#Radviz)\n",
    " * [kNN Classification](#kNN)\n",
    " * [Logistic Classification](#Logistic_Classification)\n",
    " * [PCA](#PCA)\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " Distributions\n",
    "\n",
    "* [Import modules](#Import)\n",
    "* [Get dataset](#Datasets)\n",
    "* [Archive](#Archive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest\n",
    "import bamboolib as bam\n",
    "import missingno as msno\n",
    "#import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import seaborn as sns'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'0.10.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    Problem > Data > Analysis > Model > Conclusions\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Example: Classifying Iris Species <a class=\"anchor\" id=\"iris\"></a>\n",
    "\n",
    "* [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meet the Data <a class=\"anchor\" id=\"iris_data\"></a>\n",
    "\n",
    "* [Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import seaborn as sns'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import seaborn as sns'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import seaborn as sns'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")\n",
    "mpg = sns.load_dataset(\"mpg\")\n",
    "iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a051fac5e54c988b9f7ae63390753f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BamboolibModuleWindow(children=(Window(children=(VBox(children=(VBox(children=(Button(description='Read CSV fi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00d2506fa0149ee8765ad18a75315a4"
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width    species\n",
       "0             5.1          3.5           1.4          0.2     setosa\n",
       "1             4.9          3.0           1.4          0.2     setosa\n",
       "2             4.7          3.2           1.3          0.2     setosa\n",
       "3             4.6          3.1           1.5          0.2     setosa\n",
       "4             5.0          3.6           1.4          0.2     setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  virginica\n",
       "146           6.3          2.5           5.0          1.9  virginica\n",
       "147           6.5          3.0           5.2          2.0  virginica\n",
       "148           6.2          3.4           5.4          2.3  virginica\n",
       "149           5.9          3.0           5.1          1.8  virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualization for the data\n",
    "p=msno.bar(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " msno.dendrogram(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    Info\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    Head\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    Describe\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features = iris.select_dtypes('number')\n",
    "iris_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Distributions\"></a>\n",
    "\n",
    "<h1 style=\"\n",
    "    background-color:dodgerblue; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: dodgerblue\">\n",
    "    Distributions\n",
    "</h1>\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.scatter_matrix(penguins, color='species')\n",
    "\n",
    "\n",
    "\n",
    "def show_in_window(fig):\n",
    "    import sys, os\n",
    "    import plotly.offline\n",
    "    from PyQt5.QtCore import QUrl\n",
    "    from PyQt5.QtWebEngineWidgets import QWebEngineView\n",
    "    from PyQt5.QtWidgets import QApplication\n",
    "    \n",
    "    plotly.offline.plot(fig, filename='name.html', auto_open=True)\n",
    "    \n",
    "    app = QApplication(sys.argv)\n",
    "    web = QWebEngineView()\n",
    "    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"name.html\"))\n",
    "    web.load(QUrl.fromLocalFile(file_path))\n",
    "    web.show()\n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "\n",
    "show_in_window(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = sns.load_dataset(\"mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.displot(\n",
    "    penguins, \n",
    "    x=\"flipper_length_mm\", \n",
    "    hue=\"sex\",\n",
    "    #row = \"island\",\n",
    "    col=\"species\",\n",
    "    kind = \"kde\", #\"hist\"the default or \"kde\" or \"ecdf\" univariate-only\n",
    "    #kde=True,\n",
    "    rug = True,\n",
    "    height=4, \n",
    "    aspect=.7,\n",
    "    )\n",
    "\n",
    "g.set_axis_labels(\"Density (a.u.)\", \"Flipper length (mm)\")\n",
    "g.set_titles(\"{col_name} penguins\")\n",
    "g.fig.suptitle('IT IS ALL ABOUT PENGUINS', y=1.01) # can also get the figure from plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of data points: n_data\n",
    "n_data = iris.shape[0]\n",
    "\n",
    "# Number of bins is the square root of number of data points: n_bins\n",
    "n_bins = np.sqrt(n_data*2)\n",
    "\n",
    "# Convert number of bins to integer: n_bins\n",
    "n_bins = int(n_bins)\n",
    "for feature in iris_features:\n",
    "    sns.displot(iris, x=feature, hue=\"species\",element=\"step\", kde = True, bins = n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.plot.kde(by=\"species\", figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.groupby(\"species\").plot(subplots = 4, kind='kde', figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = iris.copy()\n",
    "\n",
    "for feature in iris_features:\n",
    "    g = sns.displot(\n",
    "        data=df, \n",
    "        x = feature, \n",
    "        hue=\"species\",\n",
    "        #row = \"island\",\n",
    "        col=\"species\",\n",
    "        kind = \"kde\", #\"hist\"the default or \"kde\" or \"ecdf\" univariate-only\n",
    "        #kde=True,\n",
    "        rug = True,\n",
    "        height=4, \n",
    "        aspect=.7,\n",
    "        )\n",
    "\n",
    "    g.set_axis_labels(\"Density (a.u.)\", feature)\n",
    "    g.set_titles(\"{col_name}\", feature)\n",
    "    g.fig.suptitle('IT IS ALL ABOUT IRIS', y=1.01) # can also get the figure from plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cumulative distribution function (CDF) Plot with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = iris[\"sepal_length\"]\n",
    "\n",
    "def ecdf(x):\n",
    "    x = np.sort(x)\n",
    "    def result(v):\n",
    "        return np.searchsorted(x, v, side='right') / x.size\n",
    "    return result\n",
    "\n",
    "xs = np.unique(data)\n",
    "steps=ecdf(data)(np.unique(data))\n",
    "\n",
    "xs1 = np.column_stack([np.insert(xs,0,xs[0]-np.diff(xs).mean()),\n",
    "                       np.append(xs, xs[-1]+np.diff(xs).mean())])\n",
    "steps1 = np.concatenate([[[0,0]], np.column_stack([steps,steps])])\n",
    "\n",
    "xs2 = np.column_stack([xs,xs])\n",
    "steps2 = np.column_stack([np.insert(steps, 0, 0)[:-1], steps])\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(len(steps)+1):\n",
    "    fig.add_scatter(x=xs1[i],y=steps1[i], mode='lines', line_color='#367588', name='')\n",
    "for i in range(len(steps)):\n",
    "    fig.add_scatter(x=xs2[i],y=steps2[i], mode='lines', line_color='#367588', line_dash='dot', name='')\n",
    "fig.add_scatter(x=xs, y=steps, mode='markers', marker_color='#367588', name='')\n",
    "fig.add_scatter(x=xs, y=np.pad(steps,1)[:-2], mode='markers', marker_color='white', marker = dict(line_color='#367588', line_width=1), name='')\n",
    "fig.layout.update(title='Empirical CDF', showlegend=False)\n",
    "fig.show() #renderer='svg', height=600, width=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"CDF\"></a>\n",
    "\n",
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    CDF (cumulative distribution function)\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in iris_features:\n",
    "    sns.displot(iris, x=feature, hue=\"species\",kind=\"ecdf\")\n",
    "\n",
    "iris['species'] = iris['species'].astype('category')\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Scatter_Matrix\"></a>\n",
    "\n",
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    Scatter Matrix\n",
    "</h1>\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter_matrix(iris, color='species', template='plotly_white', opacity=0.3, title = \"Iris Scatter Matrix\")\n",
    "fig.update_layout(autosize=True, font_size=8)\n",
    "fig.update_layout({\"xaxis\"+str(i+1): dict(tickangle = -45) for i in range(10)})\n",
    "fig.update_layout({\"yaxis\"+str(i+1): dict(tickangle = -45) for i in range(10)})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(iris, hue=\"species\",diag_sharey=True)\n",
    "\n",
    "g.map_upper(sns.scatterplot, s=10, alpha=0.7)\n",
    "g.map_lower(sns.kdeplot, alpha=0.7)\n",
    "g.map_diag(sns.kdeplot, lw=1)\n",
    "\n",
    "g.add_legend()\n",
    "g.fig.subplots_adjust(top=0.9) # adjust the Figure in rp\n",
    "g.fig.suptitle('IRIS - Scatter Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species_label'] = iris['species'].factorize(sort=False, na_sentinel=-1)[0]\n",
    "iris\n",
    "\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "colors_palette = {0: \"red\", 1: \"green\", 2: \"blue\"}\n",
    "colors = [colors_palette[c] for c in iris[\"species_label\"]]   \n",
    "scatter_matrix(iris, alpha=0.5, figsize=(15, 15), diagonal='kde', color=colors, grid = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Boxplots\"></a>\n",
    "\n",
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    Boxplots\n",
    "</h1>\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]\n",
    "selection.boxplot(by=\"species\", figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupies = iris.groupby(\"species\")\n",
    "\n",
    "groupies.boxplot(figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features = iris[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_mean = iris.groupby(['species']).agg({col: ['median'] for col in iris.select_dtypes('number').columns})\n",
    "iris_mean.columns = ['_'.join(multi_index) for multi_index in iris_mean.columns.ravel()]\n",
    "iris_mean = iris_mean.reset_index()\n",
    "iris_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_cl = iris.drop(columns=['species_label'])\n",
    "iris_cl\n",
    "import plotly.express as px\n",
    "fig = px.box(iris_cl, color='species', template='plotly_white')\n",
    "fig.update_layout(autosize=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Radviz\"></a>\n",
    "\n",
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    Radviz\n",
    "</h1>\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import radviz\n",
    "\n",
    "pd.plotting.radviz(iris,'species', colormap = \"gist_rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris_eda = iris.drop(columns=['species_label', 'code'], errors = \"ignore\")\n",
    "iris_eda\n",
    "\n",
    "pd.pivot_table(iris_eda, columns=['species'], aggfunc=['mean',\"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parralel Coordinates\n",
    "\n",
    "Parallel coordinates is a plotting technique for plotting multivariate data. It allows one to see clusters in data and to estimate other statistics visually. Using parallel coordinates points are represented as connected line segments. Each vertical line represents one attribute. One set of connected line segments represents one data point. Points that tend to cluster will appear closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = iris.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "parallel_coordinates(df, \"species\")\n",
    "plt.title('Parallel Coordinates Plot', fontsize=20, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Features values', fontsize=15)\n",
    "plt.legend(loc=1, prop={'size': 15}, frameon=True,shadow=True, facecolor=\"white\", edgecolor=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"kNN\"></a>\n",
    "\n",
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 40px;\n",
    "    padding: 4px;\n",
    "    border-style: solid;\n",
    "    border-width: 4px 4px;\n",
    "    border-color: black\">\n",
    "    Machine learning\n",
    "</h1>\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model, predict and solve\n",
    "Now we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n",
    "\n",
    "Logistic Regression\n",
    "KNN or k-Nearest Neighbors\n",
    "Support Vector Machines\n",
    "Naive Bayes classifier\n",
    "Decision Tree\n",
    "Random Forrest\n",
    "Perceptron\n",
    "Artificial neural network\n",
    "RVM or Relevance Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE = LabelEncoder()\n",
    "groups = LE.fit_transform(iris['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"kNN\"></a>\n",
    "\n",
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    kNN Classification\n",
    "</h1>\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['species'] = iris['species'].astype('category')\n",
    "iris['species'] = iris['species'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting clasifier to the Training set\n",
    "# Loading libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define target and features\n",
    "y = iris[\"species\"]\n",
    "X = iris[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_train)*100\n",
    "print('Accuracy of our model is equal ' + str(round(accuracy, 2)) + ' %.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using cross-validation for parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of K for KNN\n",
    "k_list = list(range(1,50,2))\n",
    "# creating list of cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in k_list:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('The optimal number of neighbors', fontsize=20, fontweight='bold')\n",
    "plt.xlabel('Number of Neighbors K', fontsize=15)\n",
    "plt.ylabel('Misclassification Error', fontsize=15)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.plot(k_list, MSE)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of K for KNN\n",
    "k_list = list(range(1,50,2))\n",
    "# creating list of cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in k_list:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "data_tuples = list(zip(k_list,MSE))\n",
    "df = pd.DataFrame(data_tuples, columns=['Number of Neighbors K','Misclassification Error'])\n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.line(df.sort_values(by=['Number of Neighbors K'], ascending=[True]), x='Number of Neighbors K', y='Misclassification Error', template='plotly_white')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_k = k_list[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d.\" % best_k)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=best_k)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate learning model (k = 9)\n",
    "classifier = KNeighborsClassifier(n_neighbors=best_k)\n",
    "\n",
    "# Fitting the model\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "class_names = ['virginica', 'versicolor', 'setosa']\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"PCA\"></a>\n",
    "\n",
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    PCA\n",
    "</h1>\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "selectedX = iris_features\n",
    "\n",
    "pcaX = PCA(n_components=2)\n",
    "pcaX = pcaX.fit(selectedX)\n",
    "pcaX = pcaX.transform(X)\n",
    "print(pcaX.shape)\n",
    "\n",
    "plt.scatter(pcaX[:,0], pcaX[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Logistic_Classification\"></a>\n",
    "\n",
    "<h1 style=\"\n",
    "    background-color:Black; \n",
    "    color:White;\n",
    "    font-size: 30px;\n",
    "    padding: 2px;\n",
    "    border-style: solid;\n",
    "    border-width: 2px 2px;\n",
    "    border-color: black\">\n",
    "    Logistic Classification\n",
    "</h1>\n",
    "\n",
    "[Back to TOC](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# importing all the necessary packages to use the various classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\n",
    "from sklearn import svm  #for Support Vector Machine (SVM) Algorithm\n",
    "from sklearn import metrics #for checking the model accuracy\n",
    "from sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "prediction=model.predict(X_test)\n",
    "print('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for the features and the response variable\n",
    "X_1 = iris.drop('species', axis=1).values\n",
    "y_1 = iris['species'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_1 = le.fit_transform(y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNeighborsClassifier from sklearn.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(y_1, X_1, stratify=y_1, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try n_neighbors from 1 to 10\n",
    "neighbors_settings = range(1, 11)\n",
    "\n",
    "for n_neighbors in neighbors_settings:\n",
    "    # build the model\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    # record generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forrest\n",
    "\n",
    "y = iris[\"species\"]\n",
    "X = iris[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Model (can also use single decision tree)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "# Train\n",
    "model.fit(X, y)\n",
    "export_graphviz\n",
    "\n",
    "# Archive <a class=\"anchor\" id=\"Archive\"></a>\n",
    "\n",
    "* [Back to table of content](#toc)\n",
    "\n",
    "mglearn.plots.plot_linear_svc_regularization()\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))\n",
    "\n",
    "logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))\n",
    "\n",
    "plt.plot(logreg.coef_.T, 'o', label=\"C=1\")\n",
    "plt.plot(logreg100.coef_.T, '^', label=\"C=100\")\n",
    "plt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "plt.hlines(0, 0, cancer.data.shape[1])\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.legend()\n",
    "\n",
    "for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n",
    "    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\n",
    "    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
    "          C, lr_l1.score(X_train, y_train)))\n",
    "    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n",
    "          C, lr_l1.score(X_test, y_test)))\n",
    "    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\n",
    "\n",
    "plt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\n",
    "plt.hlines(0, 0, cancer.data.shape[1])\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "\n",
    "plt.ylim(-5, 5)\n",
    "plt.legend(loc=3)\n",
    "\n",
    "# Logistic Regression\n",
    "Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference Wikipedia.\n",
    "Note the confidence score generated by the model based on our training dataset.\n",
    "\n",
    "\n",
    "​\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred = logreg.predict(X_test)\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "acc_log\n",
    "\n",
    "\n",
    "We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n",
    "\n",
    "Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n",
    "\n",
    "Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n",
    "Inversely as Pclass increases, probability of Survived=1 decreases the most.\n",
    "This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\n",
    "So is Title as second highest positive correlation.\n",
    "\n",
    "coeff_df = pd.DataFrame(train_df.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n",
    "​\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference Wikipedia.\n",
    "\n",
    "Note that the model generates a confidence score which is higher than Logistics Regression model.\n",
    "\n",
    "# Support Vector Machines\n",
    "​\n",
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "Y_pred = svc.predict(X_test)\n",
    "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
    "acc_svc\n",
    "\n",
    "In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference Wikipedia.\n",
    "\n",
    "KNN confidence score is better than Logistics Regression but worse than SVM.\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "acc_knn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference Wikipedia.\n",
    "\n",
    "The model generated confidence score is the lowest among the models evaluated so far.\n",
    "\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "​\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "Y_pred = gaussian.predict(X_test)\n",
    "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n",
    "acc_gaussian\n",
    "\n",
    "\n",
    "The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference Wikipedia.\n",
    "\n",
    "\n",
    "\n",
    "# Perceptron\n",
    "​\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, Y_train)\n",
    "Y_pred = perceptron.predict(X_test)\n",
    "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n",
    "acc_perceptron\n",
    "\n",
    "\n",
    "# Linear SVC\n",
    "​\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, Y_train)\n",
    "Y_pred = linear_svc.predict(X_test)\n",
    "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
    "acc_linear_svc\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "​\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, Y_train)\n",
    "Y_pred = sgd.predict(X_test)\n",
    "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
    "acc_sgd\n",
    "\n",
    "\n",
    "\n",
    "This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference Wikipedia.\n",
    "\n",
    "The model confidence score is the highest among models evaluated so far.\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "​\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "acc_decision_tree\n",
    "\n",
    "The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference Wikipedia.\n",
    "\n",
    "The model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results.\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "​\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "random_forest.score(X_train, Y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "acc_random_forest\n",
    "\n",
    "Model evaluation\n",
    "We can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \n",
    "              'Decision Tree'],\n",
    "    'Score': [acc_svc, acc_knn, acc_log, \n",
    "              acc_random_forest, acc_gaussian, acc_perceptron, \n",
    "              acc_sgd, acc_linear_svc, acc_decision_tree]})\n",
    "models.sort_values(by='Score', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": test_df[\"PassengerId\"],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "# submission.to_csv('../output/submission.csv', index=False)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
    "                                    ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(clf.__class__.__name__)\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "axes[0].legend()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
